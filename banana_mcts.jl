using Printf
using POMDPs, POMDPTools
using QuickPOMDPs: QuickPOMDP

include("Bananagrams.jl")
using .Bananagrams


# Global variables
dict_file = "3000_common_words.txt"
dictionary = load_word_list(dict_file)
BANK_MAX = 8
BUNCH_TOT = 40
bunch = random_bunch(BUNCH_TOT, format="array")

# Reward values
turn_penalty = -1
leftover_penalty = -10   # per tile
none_left_reward = 100


# Define bananagrams MDP
bananagrams = QuickPOMDP(
    statetype = State,
    actiontype = Action,
    obstype = State,   # no obs in MDP, just to prevent getting a warning
    discount = 0.95,

    isterminal = function (s)
        if length(find_playable_word_list(s.tiles, s.letter_bank, s.occupied, dictionary)) == 0
            if length(s.letter_bank) == BANK_MAX || length(bunch) == 0
                return True
            end
        end
        return False
    end,

    actions = function (s)
        actions = find_playable_word_list(s.tiles, s.letter_bank, s.occupied, dictionary)
        if length(s.letter_bank) < BANK_MAX
            push!(actions, nothing)  # nothing = draw a tile
        end
        return actions
    end,

    transition = function (s, a)
        if a == nothing   # draw tile from bunch and add to bank
            tile = rand(bunch)
            deleteat!(bunch, findfirst(x->x==tile, bunch))
            push!(s.letter_bank, tile)
        else
            play_on_board(a.partial_word, a.parent_index, a.direction, s.tiles, s.letter_bank, s.occupied)
        end
        return s
    end,

    reward = function (s, a, sp)
        r = turn_penalty
        if isterminal(sp)
            num_leftover = length(bank) + length(bunch)
            if num_leftover == 0
                r += none_left_reward
            else
                r += num_leftover*leftover_penalty
            end
        end
        return r
    end,

    initialstate = init_state(),   # maybe not needed?
)


# Define MCTS struct and functions

struct MonteCarloTreeSearch
    ğ’«   # problem
    N   # dictionary of visit counts
    Q   # dictionary of action value estimates
    d   # depth
    m   # number of simulations
    c   # exploration constant
    U   # value function estimate
end

bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)
function explore(Ï€::MonteCarloTreeSearch, s)
    ğ’œ, N, Q, c = actions(Ï€.ğ’«, s), Ï€.N, Ï€.Q, Ï€.c
    Ns = sum(N[(s,a)] for a in ğ’œ)
    return argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), ğ’œ)
end

function simulate!(Ï€::MonteCarloTreeSearch, s, d=Ï€.d)
    if d <= 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ, Î³ = actions(ğ’«, s), discount(ğ’«)
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s,a)] = 0
            Q[(s,a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, s)
    sp = transition(ğ’«, s, a)
    r = reward(ğ’«, s, a, sp)
    q = r + Î³*simulate!(Ï€, sp, d-1)
    N[(s,a)] += 1
    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]
    return q
end

function (Ï€::MonteCarloTreeSearch)(s)
    for k in 1:Ï€.m
        simulate!(Ï€, s)
    end
    return argmax(a->Ï€.Q[(s,a)], actions(Ï€.ğ’«, s))
end

# Value estimate from random rollout
function U(s)
    sim = RolloutSimulator()
    policy = RandomPolicy(mdp)                  # generates actions at each state and randomly chooses one
    return simulate(sim, bananagrams, policy)   # returns reward from rollout with specified policy; ends when isterminal is True
end

N = Dict{Tuple{State, Action}, Int}()
Q = Dict{Tuple{State, Action}, Float64}()
d = 10
m = 100
c = 100    # d, m, c values used in textbook example

Ï€ = MonteCarloTreeSearch(bananagrams, N, Q, d, m, c, U)

s = init_state()
while !isterminal(Ï€.ğ’«, s):
    a = Ï€(s)   # action to take accord to MCTS
    transition(Ï€.ğ’«, s, a)
end



# # POMDPs API: https://juliapomdp.github.io/POMDPs.jl/latest/api/#API-Documentation
# println("Î³: ", discount(bananagrams))   # usage test
# println("Bunch: ", bunch)

# # Test board visualization: 
# # NOTE: saving .png file works but board does not pop up when display() is called
# my_tiles = Vector{Tile}()
# S_tile = Tile('S', (1,3), 0, 4, 0, 2)
# I_tile = Tile('I', (2,3), 0, 0, 1, 3)
# T_tile = Tile('T', (3,3), 0, 0, 2, 0)
# P_tile = Tile('P', (1,2), 1, 5, 0, 0)
# A_tile = Tile('A', (1,1), 4, 0, 0, 0)
# push!(my_tiles, S_tile)
# push!(my_tiles, I_tile)
# push!(my_tiles, T_tile)
# push!(my_tiles, P_tile)
# push!(my_tiles, A_tile)
# occupied = Set{Tuple{Int, Int}}([(1,1), (1,2), (1,3), (2,3), (3,3)])
# my_bank = ['C', 'R', 'T', 'S', 'A', 'G']
# test_state = State(my_tiles, occupied, my_bank)
# see_board(my_tiles, my_bank, save=true)
